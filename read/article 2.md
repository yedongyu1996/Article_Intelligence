# Blockchain in accounting research: current trends and emerging topics (2022)

### 1. 研究目标 · 内容 · 问题 · 出发点

-   **研究领域与背景、具体对象 / 数据集**
    -   **研究领域**：会计学与信息技术交叉领域，具体聚焦于区块链技术在会计和审计研究中的应用。
    -   **背景**：区块链作为一项颠覆性技术，开始对许多行业（包括会计业）的商业模式和市场结构产生巨大影响。然而，关于该主题的信息财富使得研究人员难以跟上最新发展，且现有文献综述覆盖范围有限。
    -   **研究对象/数据集**：论文的研究语料库包含153篇学术论文，时间跨度为2008年1月至2020年6月。数据来源包括：1) 在两大商学院期刊排名列表（ABS和ABDC）中收录的会计期刊；2) 社会科学研究网（SSRN）上发布的尚未正式出版的预印本论文。

-   **论文想解决的核心问题**
    该论文旨在系统性地回答关于会计领域中区块链研究的三个核心问题：
    1.  当前与会计相关的区块链研究的主要趋势和主题是什么？
    2.  这些关键研究主题的焦点是什么？应如何进行评述？
    3.  会计领域中区块链研究的未来趋势是什么？

-   **研究动机 / 假设**
    -   **研究动机**：鉴于区块链在会计领域的影响尚未得到充分探索，且该技术可能对会计实践产生深远变革，进行一次结构化的文献综述（SLR）是及时且必要的。作者旨在为该新兴领域的研究提供一个全面的图景、批判性分析和未来的研究方向，为学者、从业者和政策制定者提供一个有价值的参考基准。
    -   **假设**：作者假设，通过结合机器学习方法（如LDA）、引文分析和专家手动审查的混合方法，可以比传统文献综述更系统、更客观地识别出该领域的关键研究主题、新兴趋势和未来方向。

-   **工作内容概览（精炼概述各章节核心）**
    -   **第1、2节（引言与背景）**：介绍了区块链作为一项潜在的颠覆性会计技术的概念，并指出当前缺乏对该领域研究的全面综述，从而引出本文的三个研究问题。
    -   **第3节（研究方法）**：详细阐述了论文采用的结构化文献综述方法，包括论文筛选的三阶段流程，以及结合了机器学习（LDA）、引文分析和人工审查的混合分析方法。
    -   **第4节（结果）**：回答了第一个研究问题。通过LDA分析和引文分析，展示了该领域文献的年度增长趋势，并识别出十大研究主题，其中四个核心主题占据了主导地位。
    -   **第5节（关键研究主题：焦点与评述）**：回答了第二个研究问题。对LDA识别出的四个最主要的研究主题——会计师角色的变化、审计师的新挑战、区块链技术应用的机会与挑战、加密资产的监管——进行了深入的手动分析和批判性评述。
    -   **第6节（未来研究方向）**：回答了第三个研究问题。基于前文的分析，为四个核心主题分别提出了具体的未来研究议程。
    -   **第7节（结论）**：总结了研究发现，并讨论了其对学术界、会计实践和政策制定的启示，同时说明了研究的局限性。

### 2. 研究方法（含模型 / 技术详解）

-   **理论框架与算法**
    -   **理论框架**：本文采用结构化文献综述（Structured Literature Review, SLR）作为核心理论框架。SLR是一种系统地研究学术文献语料库，以产生见解、批判性反思和未来研究路径的方法。
    -   **核心技术**：为实现SLR，作者采用了一种混合方法，结合了三种互补的技术：
        1.  **潜在狄利克雷分布（Latent Dirichlet Allocation, LDA）**：一种无监督的生成式概率主题模型，用于从大规模文档集合中发现隐藏的主题结构。
        2.  **引文分析（Citation Analysis）**：一种计量学方法，通过分析文献的被引用次数来评估其学术影响力。
        3.  **人工审查（Manual Review）**：由研究人员进行的定性分析，用于深入解读和批判性评估特定文献。

-   **关键模型/技术逐一说明：架构、输入输出、训练或推理流程、优势与局限**
    -   **1. 潜在狄利克雷分布（LDA）**
        -   **架构**：LDA模型将每篇文档视为多个主题的混合体，而每个主题又是多个词汇的概率分布。
        -   **输入**：经过预处理的文本语料库，具体形式为“词袋”（bag-of-words）矩阵，其中每个文档被表示为一个词频向量。
        -   **推理流程（数据预处理与建模）**：
            1.  **文本转换**：将153篇论文的PDF文件转换为纯文本文件。
            2.  **文本清洗**：将文本全部转为小写，并移除所有非字母字符。
            3.  **标准化**：移除“the”、“and”等停用词，并对其余单词进行词形还原（lemmatisation），即将其转换为字典中的基本形式。
            4.  **特征选择**：仅保留名词，丢弃其他词性的单词。
            5.  **模型训练**：将处理后的“词袋”数据输入LDA模型进行训练。模型的主题数量通过网格搜索和主题一致性评估进行优化，最终确定为10个主题。
        -   **输出**：
            1.  10个主题，每个主题由一系列高概率的关键词及其权重来定义。
            2.  每个主题在整个语料库中的边际分布（即重要性占比）。
            3.  每篇论文与各个主题的关联度得分，从而可以识别出每个主题下最具代表性的论文。
        -   **优势**：相比简单的词频统计，LDA能够提供更系统、客观、可复制的主题识别方法，并揭示词语和主题间的潜在关联。
        -   **局限**：LDA采用“词袋”模型，忽略了单词的顺序和上下文语义。

    -   **2. 引文分析**
        -   **流程**：使用 Harzing's Publish or Perish 软件，从谷歌学术（Google Scholar）数据库中检索截至2021年3月5日的论文引用数据。
        -   **输入**：语料库中的论文列表。
        -   **输出**：
            1.  每篇论文的总引用次数。
            2.  每篇论文的年均引用次数（Citations Per Year, CPY），以消除旧文章因时间积累而引用量偏高的问题，从而更好地识别新兴热门文章。
        -   **优势**：作为一种验证手段，通过识别高影响力文章来确认LDA所发现主题的重要性，增强了研究结果的可靠性。

    -   **3. 人工审查**
        -   **流程**：由三位作者共同进行。首先由一位作者为LDA识别出的每个主题起草描述性标题，然后由其他作者审查和修改。随后，团队对LDA识别出的每个核心主题下最具代表性的15篇文章进行详细阅读和定性分析。
        -   **输入**：LDA模型输出的主题和代表性文章列表。
        -   **输出**：对每个核心主题的深入批判性评述、对未来趋势的见解以及详细的研究方向建议。
        -   **优势**：弥补了纯机器学习方法的不足。人类研究者能更好地评估文献的细微差别、进行批判性思考，并预测未来趋势。

### 3. 实验设计与结果（含创新点验证）

本文的“实验”即其文献分析流程。

-   **实验流程（足够详细便于复现）**
    1.  **第一步：文献语料库构建**
        -   **期刊选择**：首先合并2018年ABS和2019年ABDC两个会计期刊排名列表，去重后得到149本目标期刊。
        -   **数据库检索**：在EBSCO, Scopus, 和 Web of Science数据库中，对这149本期刊进行检索。检索时间范围为2008年1月至2020年6月，关键词为标题或摘要中包含 "blockchain" 或 "distributed ledger technology"。此步初步筛选出112篇论文。
        -   **边界扩展**：为纳入最新的研究成果以克服学术出版的延迟，研究人员决定扩展检索范围。在SSRN（社会科学研究网）上使用相同时间段和关键词（"accounting" AND "blockchain" 或 "accounting AND distributed ledger"）进行检索，获得68篇论文。
        -   **最终样本筛选**：排除与期刊论文重复的部分，并排除那些后续发表在非会计期刊或未被ABS/ABDC收录的会计期刊上的论文，最终得到41篇额外的SSRN论文。最终样本由112篇期刊论文和41篇SSRN论文组成，共计153篇。

    2.  **第二步：混合方法分析**
        -   将153篇论文的PDF文件作为输入，执行上文“研究方法”部分详述的 **LDA主题建模** 流程，识别出10个主题及其分布。
        -   执行 **引文分析**，收集高被引论文数据，用于验证LDA结果的有效性。
        -   对LDA识别出的四个最主要的主题（合计占比超过50%）进行深入的 **人工审查**，进行批判性分析和未来方向的探讨。

-   **数据集、参数、评价指标**
    -   **数据集**：153篇关于区块链与会计的学术论文。
    -   **参数**：LDA模型的主题数（k）被设置为10。
    -   **评价指标**：
        -   **LDA模型**：主题一致性（Topic Coherence）用于优化主题数量。
        -   **文献趋势**：年度发文量。
        -   **主题重要性**：每个主题的边际主题分布（Marginal Topic Distribution）百分比。
        -   **论文影响力**：总引用次数和年均引用次数（CPY）。
        -   **方法有效性**：LDA识别出的核心主题与高被引论文主题之间的一致性。

-   **创新点如何得到验证，结果对比与可视化描述**
    -   **创新点验证**：本文方法论的创新（混合方法）通过结果的一致性得到验证。**引文分析的结果（表3和表4）强有力地支持了LDA分析的结果（表2）**。具体表现为，被引用次数最多和年均引用次数最高的文章，其研究内容与LDA识别出的四个最主要主题（会计师角色变化、审计师新挑战、应用机遇与挑战、加密资产监管）高度吻合。这种交叉验证表明，该混合方法能够准确地捕捉到该研究领域的核心和热点议题。

    -   **结果与可视化描述**：
        -   **图1（年度文章数量）**：该折线图清晰地显示，从2015年出现零星文章开始，到2017年后，相关研究论文数量呈指数级增长趋势，尤其是在2019年和2020年上半年达到顶峰，证明该领域的研究热度迅速攀升。
        -   **表2（LDA主题列表）**：该表列出了LDA识别的10个主题、每个主题下的前20个关键词以及该主题在整个语料库中的占比。结果显示，排名前四的主题（会计师角色变化24%，审计师新挑战16%，应用机遇与挑战11%，加密资产监管11%）合计占比超过60%，是该领域绝对的研究核心。
        -   **图2（各主题的发表趋势）**：“会计师角色变化”和“审计师新挑战”两个主题的讨论热度在2016至2020年间持续上升，而“金融科技在银行业的应用”和“加密货币与加密资产”等早期主题的兴趣则呈现下降趋势，揭示了研究焦点的动态演变。

-   **主要实验结论与作者解释**
    -   区块链在会计领域的研究虽然发展迅速，但尚未成为主流，顶级期刊发表的文章很少。
    -   现有文献大多是规范性（normative）的，即探讨“应该如何”或“未来可能如何”，而非基于真实案例的实证研究。
    -   四个最核心的研究领域被成功识别，为后续的深入分析提供了焦点。
    -   该混合分析方法是有效的，能够系统、客观地勾勒出一个新兴研究领域的全貌。

### 4. 研究结论

-   **重要发现（定量 / 定性）**
    1.  **四大核心研究主题**：通过系统分析，论文确定了会计领域区块链研究的四大支柱：会计师角色的演变、审计师面临的新挑战、区块链技术应用的机遇与挑战，以及加密资产的监管问题。这四个主题占据了超过一半的研究文献。
    2.  **研究范式**：当前该领域的研究绝大多数是规范性的和概念性的，缺乏基于实际应用场景的实证研究和案例分析。
    3.  **职业演变而非替代**：研究普遍认为，区块链不会完全取代会计师和审计师，而是会增强他们的能力，将其角色从重复性的数据记录和核对工作，转变为更具战略性、判断性和咨询性的高级顾问角色。
    4.  **技能缺口**：会计和审计专业人员需要发展新的技能，特别是信息技术、数据分析、系统思维和专业判断能力，以适应新技术环境。
    5.  **监管滞后**：在加密资产的会计确认、计量和报告方面，存在显著的监管空白和法律框架缺失，这是阻碍其健康发展和应用的主要障碍。

-   **对学术或应用的意义**
    -   **对学术的意义**：
        -   为该新兴领域提供了一份全面的、结构化的“研究地图”，帮助研究人员快速了解现状、识别研究前沿和空白。
        -   提出的详细未来研究议程，可直接启发后续的学术研究方向，并可能促进期刊编辑设立相关特刊。
        -   展示了一种结合机器学习和人工分析的强大文献综述方法，可被其他领域的学者借鉴。
    -   **对应用的意义**：
        -   **对会计从业者**：揭示了未来职业发展的方向，强调了技能转型的紧迫性，帮助他们为即将到来的行业变革做好准备。
        -   **对教育机构**：明确了当前会计教育与未来市场需求之间的差距，呼吁高校改革会计和金融课程，融入更多关于新兴技术、数据分析和战略思维的内容。
        -   **对政策制定者和监管机构**：强调了为区块链技术和加密资产制定清晰的法律、会计和税收框架的紧迫性，以促进技术创新、保护投资者并维护市场稳定。

### 5. 创新点列表

1.  **方法论创新**：首次在会计文献综述研究中，系统地采用了一种结合了机器学习（LDA）、引文分析和专家人工审查的混合研究方法，提高了文献分析的系统性、客观性和深度。
2.  **全面的文献覆盖**：构建了迄今为止该主题最全面的研究语料库（153篇），远超之前仅覆盖数十篇文献的综述，提供了更完整的领域图景。
3.  **前瞻性的数据源**：创新性地将SSRN的预印本论文纳入分析，能够比传统仅依赖已发表期刊论文的综述更早地捕捉到新兴的研究趋势和热点。
4.  **系统性的主题识别与批判**：不仅识别出研究主题，还对四个核心主题进行了深入的批判性分析，并为每个主题提供了结构化、可操作的未来研究议程。
5.  **动态趋势的可视化**：通过图表清晰地展示了研究热度的整体增长趋势以及各个具体主题随时间演变的动态，为理解该领域的发展轨迹提供了直观的证据。

=============================《文章分隔符》=============================

# Multidimensional Scientometric indicators for the detection of emerging research topics (2021)

### 1. 研究目标 · 内容 · 问题 · 出发点
- **研究领域与背景、具体对象 / 数据集**
  - **研究领域**：科学计量学、技术预测与社会变迁、科技政策。
  - **研究背景**：及时准确地识别新兴研究主题（Emerging Research Topics, ERT）对于科研基金和政策制定者优化资源配置、促进有前景的研究至关重要。
  - **具体对象**：本研究以“干细胞研究”领域作为案例，展示其提出的ERT识别方法的有效性。
  - **数据集**：使用了Web of Science (WoS) 数据库中的422,101篇干细胞研究论文和PATSTAT数据库中的50,556项干细胞相关专利，时间跨度为2004年至2018年。

- **论文想解决的核心问题**
  1.  **定义模糊**：ERT的概念以及其与研究前沿（Frontier Topics, FT）、颠覆性主题（Transformative Topics, TT）等相关概念的界限模糊，缺乏一个清晰、可操作的定义。
  2.  **指标片面**：以往的ERT识别方法主要关注“新颖性”和“增长性”这两个属性，而忽略了同样重要的“潜在社会经济影响”和“不确定性”等维度。
  3.  **缺乏实用框架**：缺少一个能够将多维属性操作化、并在细粒度上识别ERT的系统性框架和流程。

- **研究动机 / 假设**
  - **研究动机**：为了更全面、准确地辅助科技决策，需要一个超越传统指标的、能够综合考量ERT多重特性的新方法。
  - **核心假设**：一个成功的ERT不仅应具备新颖和快速增长的特点，还应表现出持续性、连贯性，并具有潜在的高社会经济影响和随时间发展的“不确定性降低”趋势。一个综合衡量这五个维度的多维指标框架，可以更有效地识别出真正有价值的ERT。

- **工作内容概览**
  - **第一部分（理论背景）**：通过定性和定量方法（如出版物趋势、VOSviewer覆盖图分析）对ERT、FT、TT等概念进行辨析。回顾了现有的ERT识别方法，将其分为基于引文、基于词汇和混合方法三类。
  - **第二部分（方法论）**：构建了一个识别ERT的理论框架，该框架将ERT的特征定义为五个维度：根本新颖性、相对快速增长、持续性与连贯性、潜在高影响以及不确定性与模糊性降低。并为这五个维度设计了一套可操作的多维科学计量指标。
  - **第三部分（案例研究）**：将所提出的方法应用于干细胞研究领域。详细描述了从数据采集、主题发现、多轮筛选到最终识别出26个ERT的完整流程，并对结果进行了验证和分析。
  - **第四部分（结论与讨论）**：总结了研究的主要发现和贡献，分析了研究的局限性，并提出了未来研究方向。特别地，根据识别出的ERT的不同特征模式，提出了相应的七种研发（R&D）布局策略。

### 2. 研究方法（含模型 / 技术详解）
- **理论框架与算法**
  - **理论框架**：本研究的框架分为两步：1) **主题发现**：从大规模文献数据中识别出有意义的研究主题；2) **ERT识别**：利用一套多维指标体系，从发现的主题中筛选出符合ERT特征的主题。
  - **核心算法**：
    - **Leiden算法**：用于主题发现。这是一种先进的社区发现算法，应用于WoS数据库的直接引文网络。它能在保证社区内部良好连接性的前提下，将海量出版物划分到不同的层级（宏观、中观、微观）。本研究使用其“微观”层面的分类结果（4047个研究领域）来识别干细胞领域内的具体主题。

- **关键模型/技术逐一说明**
  本研究的核心在于一套多维度的ERT识别指标体系，分为“门槛指标”和“参考指标”。

  - **门槛指标 (Threshold Indicators)**
    - **(1) 新颖性与增长性 (Novelty and Growth)**
      - **架构**：通过计算四个子指标的平均增长率 (AGR) 来衡量一个主题的活跃度和新颖程度。这四个指标被视为识别ERT的基础门槛。
      - **输入**：特定主题在连续时间窗口内的论文数、期刊数、基金资助数、作者数。
      - **输出**：四个维度的AGR值。
      - **流程**：使用特定公式计算AGR，近期增长率被赋予更高权重。AGR持续为负或关键指标为空的主题被过滤掉。
      - **优势**：多角度衡量增长，比单一的论文增长率更稳健。
      - **公式**: 平均增长率 (AGR)
        $$V = \frac{1}{N_r} \sum \frac{N_{t} - N_{t-1}}{N_{t-1}}$$
        其中，$V$ 是AGR，$N_r$ 是计算的期数，$N_t$ 是在第 $t$ 个时间区间的实体数量（论文、期刊等）。

    - **(2) 持续性与连贯性 (Persistence and Coherence)**
      - **架构**：通过主题演化图和相似性计算来评估。一个主题若能在多个连续时间窗口内保持存在并增强其内在关联，则被认为具有持续性和连贯性。
      - **输入**：相邻时间窗口内，同一主题下的出版物集合。
      - **输出**：杰卡德相似度 (Jaccard Similarity) 分数和演化路径图。
      - **流程**：计算相邻时间窗口间主题的杰卡德相似度。相似度持续增加表明主题的连贯性在增强。
      - **优势**：量化了主题的稳定性和内在聚合度，过滤掉昙花一现的热点。
      - **公式**: 杰卡德相似度
        $$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

  - **参考指标 (Reference Indicators)**
    - **(3) 潜在高影响 (Potential Prominent Impact)**
      - **架构**：通过分析科学（论文）与技术（专利）之间的相互引文关系，衡量主题的潜在社会经济价值。
      - **输入**：特定主题的论文集、专利集，以及它们之间的相互引文数据。
      - **输出**：两个关键指标 $U_{BA}$ 和 $U_{AB}$。
      - **流程**：计算从专利到论文的引文（代表基础科学向应用科学转化）和从论文到专利的引文（代表应用科学支撑基础科学）在主题总论文中的占比。
      - **优势**：超越了纯学术影响力的评估，直接关联到技术应用和经济潜力。
      - **公式**:
        - 基础科学到应用科学转化能力: $U_{BA} = \frac{\text{被专利引用的论文数}}{\text{主题总论文数}}$
        - 应用科学到基础科学转化能力: $U_{AB} = \frac{\text{引用了专利的论文数}}{\text{主题总论文数}}$

    - **(4) 不确定性与模糊性降低 (Reduction to Uncertainty and Ambiguity)**
      - **架构**：通过衡量一个主题对整个知识网络结构的影响力来评估其发展前景的确定性。一个能显著影响网络结构的主题，其未来发展路径更清晰，不确定性更低。
      - **输入**：特定领域内所有主题构成的引文网络。
      - **输出**：两个网络拓扑变化指标 $\Delta N_{SC}$ 和 $\Delta N_{WC}$。
      - **流程**：从整个知识网络中逐一移除某个候选ERT，然后计算网络的强连通分量 (Strongly Connected Components) 和弱连通分量 (Weakly Connected Components) 数量的变化率。
      - **优势**：为“不确定性”这一抽象概念提供了创新的量化方法，通过网络拓扑变化来预测主题的未来核心地位。
      - **指标**:
        - $\Delta N_{SC}$: 强连通分量数量的平均增长率。
        - $\Delta N_{WC}$: 弱连通分量数量的平均增长率。
        一个理想的ERT应能显著增加强连通性（成为知识核心），同时其不确定性会降低。

### 3. 实验设计与结果（含创新点验证）
- **实验流程**
  1.  **数据采集**：从WoS和PATSTAT数据库获取2004-2018年干细胞领域的论文和专利数据。将时间划分为11个重叠的5年期子区间（如2004-2008, 2005-2009...）。
  2.  **主题发现与初筛**：
      - 使用Leiden算法的微观分类，识别出干细胞领域的主题。
      - 在每个时间区间内，选取相关性最高的Top 50主题，合并后得到 **65个** 唯一的候选主题。
  3.  **多轮过滤**：
      - **持久性过滤**：移除在少于3个时间区间内出现的主题，剩余 **57个**。
      - **趋势过滤**：移除那些短暂增长后迅速衰退的主题，剩余 **54个**。
      - **增长性门槛过滤 (Step 1)**：
        - 计算所有主题在全部11个区间的四项AGR（论文、期刊、基金、作者）。移除在期刊、基金、作者上有空值的主题，剩余 **44个**。
        - 仅考察 **最近5个时间区间** 的论文AGR，移除近期呈负增长的主题。最终得到 **26个** 候选ERT。
  4.  **指标计算与分析**：
      - 对这26个ERT，计算它们的 **持续性与连贯性**（杰卡德相似度）、**潜在高影响**（$U_{BA}$ 和 $U_{AB}$）以及 **不确定性降低**（$\Delta N_{SC}$ 和 $\Delta N_{WC}$）指标。
  5.  **结果验证与解释**：
      - 将识别出的26个ERT（如“细胞外基质对干细胞命运的调控”、“脱细胞支架在组织再生医学中的应用”）与该领域的权威综述、年度科学突破新闻（如发表在 *Cell*, *Nature* 上的成果）进行对比。

- **数据集、参数、评价指标**
  - **数据集**：WoS干细胞论文 (422,101篇)，PATSTAT干细胞专利 (50,556项)。
  - **参数**：
    - 时间窗口：5年，滑动步长1年。
    - Top N主题：每个窗口选Top 50。
    - 持久性阈值：至少在3个窗口中出现。
    - 杰卡德相似度阈值（可视化用）：>= 0.04。
  - **评价指标**：
    - 内部指标：8个二级指标（4个AGR，$U_{BA}$, $U_{AB}$, $\Delta N_{WC}$, $\Delta N_{SC}$）。
    - 外部验证：与领域专家的定性证据（新闻、综述文章）进行比对，验证识别结果的有效性。

- **创新点如何得到验证，结果对比与可视化描述**
  - **创新点验证**：
    - **多维框架的有效性**：实验识别出的26个ERTs被重新打上语义化标签后（见附录表A5），与干细胞领域的重大进展新闻（见表10）高度吻合。例如，识别出的主题353（细胞外基质）和814（脊髓损伤）等，都在2018年有顶级期刊的突破性研究发表，证明了该方法的预测能力。
    - **指标独立性**：对8个二级指标进行皮尔逊相关性分析（见表9），发现它们之间大多不相关，这反过来证明了使用多维指标的必要性，因为每个指标都从不同角度捕捉了ERT的特征。
  - **结果对比**：本研究与Wang (2018)的研究不同，后者同时考虑所有科学领域，而本研究聚焦于特定领域以获取更深入信息。与Porter et al. (2019)也不同，本研究直接识别ERT而非依赖复杂的黑箱模型。最关键的区别在于，本研究明确地将“潜在影响”和“不确定性”这两个被以往研究忽视的维度操作化并纳入框架。
  - **可视化描述**：
    - **图7 & 图8**: 使用演化图展示了ERTs随时间的持续性和连贯性。图中每个主题都有一条清晰的演化轨迹，且在后期（2013-2017之后）杰卡德相似度（连贯性）普遍急剧上升，直观地验证了这些主题的聚合趋势。
    - **图9**: 展示了早期（2004-2008）主题间的引文网络结构，为计算不确定性指标提供了基础。

- **主要实验结论与作者解释**
  - 实验成功在干细胞领域识别出26个具有高新颖性和增长性的ERT。
  - 这些ERT在“潜在影响”和“不确定性”两个参考指标上表现各异，这表明即使都是ERT，其发展模式和政策需求也不同。
  - 基于这三个主要维度的表现，作者将26个ERT分为7种模式（如NG-PI-UR型，即三高型），并为每种模式提出了具体的R&D布局建议。例如，对于三高型主题（如Topic 1046），建议优先布局和重点投资；而对于仅新颖性高（NG型）的主题，则建议在适当布局的同时，加速成果转化并监控其发展不确定性。这为政策制定提供了直接的、可操作的指导。

### 4. 研究结论
- **重要发现（定量 / 定性）**
  1.  **方法有效性**：本研究提出的多维指标框架能够有效地在特定领域内以细粒度识别出ERT，并且识别结果得到了外部定性证据的支持。
  2.  **ERT模式划分**：ERT并非单一模式，可以根据新颖增长性（NG）、潜在影响（PI）和不确定性降低（UR）这三个维度的不同组合，划分为七种不同的发展模式。
  3.  **指标独立性**：构成框架的多个指标在很大程度上是相互独立的，这强调了综合评估的必要性和优越性。
  4.  **策略应用**：不同的ERT模式对应不同的R&D布局策略，从而使科技政策的制定更具针对性和前瞻性。

- **对学术或应用的意义**
  - **学术意义**：
    - 首次尝试将ERT的五个关键属性（包括潜在影响和不确定性）进行全面的操作化和实证分析，丰富了ERT的理论内涵。
    - 为“不确定性”等抽象概念提供了创新的量化思路，推动了科学计量学方法论的发展。
    - 通过定量方法对ERT、FT、TT等概念进行了辨析，为相关研究提供了更清晰的概念基础。
  - **应用意义**：
    - 为政府、科研资助机构等决策者提供了一个灵活、可定制的工具，用于评估和优先排序科研方向。
    - 提出的“ERT模式-R&D策略”对应框架，为科技政策的制定和创新资源的优化配置提供了直接、具体的决策支持。

### 5. 创新点列表
1.  **首创全面的ERT五维框架**：首次设计并操作化了一个综合衡量新兴研究主题（ERT）五大特征的框架，即：根本新颖性、相对快速增长、持续性与连贯性、潜在高影响、以及不确定性与模糊性降低。
2.  **量化“不确定性”维度**：创新性地提出通过测量一个主题对知识网络拓扑结构（强/弱连通分量）的影响来量化其“不确定性降低”的程度，为这一抽象概念的评估提供了可行的计算方法。
3.  **深化“潜在影响”的衡量**：通过对科学论文与技术专利间的**双向**引文关系（即论文引专利、专利引论文）进行综合分析，更全面地评估了ERT的潜在社会经济影响，超越了传统的单向分析。
4.  **清晰化相关概念**：结合定性讨论与定量分析（出版物趋势、覆盖网络图、词频分析），系统地辨析了ERT与研究前沿（FT）、颠覆性主题（TT）等易混淆概念之间的区别与联系。
5.  **提出 actionable 的R&D策略**：基于多维指标的评估结果，将识别出的ERTs划分为七种不同的发展模式，并为每种模式提出了具体、可操作的R&D布局策略，极大地增强了研究的现实应用价值和政策指导意义。

=============================《文章分隔符》=============================

# Combining topic modeling and SAO semantic analysis to identify technological opportunities of emerging technologies (2021)

### 1. 研究目标 · 内容 · 问题 · 出发点

* **研究领域与背景、具体对象 / 数据集**
    * **研究领域与背景**: 本研究聚焦于技术机会分析（Technological Opportunities Analysis, TOA），旨在为国家和企业在科技创新中识别具有竞争优势的潜在技术。传统的专家判断方法存在主观性和局限性，而现有的文本挖掘和文献计量方法在揭示技术主题间的深层语义关系方面存在不足。
    * **具体对象 / 数据集**: 论文以染料敏化太阳能电池（Dye-Sensitized Solar Cell, DSSC）技术为案例，分析了从德温特创新索引（Derwent Innovation Index, DII）数据库中检索到的 9,883 项相关专利。

* **论文想解决的核心问题**
    * 如何更准确地从专利文本中提取技术主题？现有方法或因仅使用专利分类码（如 IPC）而过于粗略，或因仅使用文本术语而忽略了分类码的价值。
    * 如何揭示不同技术主题（而非单个技术点）之间的深层语义联系？传统的共词分析等方法无法揭示技术间的具体关系（如“改进”、“构成”），而现有的 SAO（主语-动作-宾语）分析仅能连接“点对点”的技术，而非“主题对面”的技术簇。

* **研究动机 / 假设**
    * **动机**: 随着新兴技术的快速发展，准确及时地识别技术机会成为保持创新优势的关键。现有 TOA 方法存在缺陷，需要一种更系统、准确的方法来识别潜力技术及其演进路径。
    * **假设**:
        1.  通过结合专利的文本术语（标题、摘要）和分类码（IPC、DMC），并为标题中的术语赋予更高权重，可以构建一个更精确的技术主题提取模型。
        2.  通过将主题模型、SAO 语义分析和机器学习相结合，可以克服现有方法的局限性，有效挖掘出技术主题之间的隐藏语义关系，从而构建技术路线图并发现技术机会。

* **工作内容概览（精炼概述各章节核心）**
    * **第一阶段：数据预处理**: 从 DII 数据库检索并清洗 DSSC 专利数据，提取标题、摘要、IPC 和 DMC 等特征。
    * **第二阶段：优化主题提取模型构建**: 设计并验证一个特征选择模型，通过对比不同特征组合（标题、摘要、IPC、DMC、加权标题）的性能，构建出最优的基于 LDA 的主题提取模型。
    * **第三阶段：核心与潜力技术识别**: 应用优化后的模型从全部专利中提取技术主题，并通过计算主题权重（TW）和主题趋势（TT）两个指标，建立二维评估体系以识别核心技术和潜力技术。
    * **第四阶段：技术路线图构建与机会发现**: 使用 SAO 语义分析和机器学习（朴素贝叶斯分类器）来识别和分类不同技术主题间的语义关系（构成、改进、替代、应用），最终构建 DSSC 的技术路线图（TRM），并据此揭示技术演化路径和潜在机会。

### 2. 研究方法（含模型 / 技术详解）

* **理论框架与算法**
    * **理论框架**: 研究借鉴了托马斯·库恩的科学革命理论，将技术发展视为一个发现“异常”、产生新范式的过程，旨在识别那些有潜力解决未来问题的“异常”作为技术机会。整个研究框架是一个结合了主题模型、SAO 语义分析、机器学习和专家判断的四阶段混合方法。
    * **核心算法**:
        * **潜在狄利克雷分布（Latent Dirichlet Allocation, LDA）**: 用于从专利文献中生成潜在的技术主题。
        * **SAO 语义分析**: 用于从专利文本中提取“主语-动作-宾语”结构，以揭示技术实体间的关系。
        * **朴素贝叶斯分类器**: 一种监督学习算法，用于对 SAO 结构所代表的语义关系进行自动分类。

* **关键模型/技术逐一说明：架构、输入输出、训练或推理流程、优势与局限**
    * **1. 优化的 LDA 主题提取模型**
        * **架构**: 该模型的核心是 LDA，但其创新在于对 LDA 的输入数据进行了优化。研究设计了一个特征选择模型，测试了 8 种不同的特征组合（如下表所示），以确定最佳输入。
            | 组 | 模型编号 | 模型结构 |
            | :-- | :--- | :--- |
            | A | #1 | 标题 + 摘要术语 |
            | A | #2 | 标题 + 摘要术语 + IPC |
            | A | #3 | 标题 + 摘要术语 + DMC |
            | A | #4 | 标题 + 摘要术语 + IPC + DMC |
            | B | #1 | 加权标题 (2倍) + 摘要术语 |
            | B | #2 | 加权标题 (2倍) + 摘要术语 + IPC |
            | B | #3 | 加权标题 (2倍) + 摘要术语 + DMC |
            | B | #4 | 加权标题 (2倍) + 摘要术语 + IPC + DMC |
        * **输入**: “文档-特征”矩阵。矩阵的行代表专利文档，列代表特征（词语、IPC、DMC），单元格的值为特征在文档中出现的频率。对于 B 组模型，标题中术语的频率会乘以 2。
        * **输出**:
            1.  每个文档的主题概率分布（一个 M×K 的矩阵，M 为文档数，K 为主题数）。
            2.  每个主题的词语概率分布。
        * **流程**:
            1.  使用 Gibbs 采样对 LDA 模型进行参数推断。
            2.  通过计算困惑度（Perplexity）来确定最佳主题数 K。困惑度越低，模型拟合效果越好。
            3.  通过在带标签的训练集上计算总精度（Total Precision, TP），评估 8 种特征组合的性能，选择 TP 最高的模型作为最终模型。
        * **优势**: 相比传统方法，该模型能处理主题高度耦合的文档集合，解决一词多义问题，并通过优化输入特征显著提高了主题识别的准确性。

    * **2. 结合 SAO 与机器学习的语义关系分析**
        * **架构**: 这是一个多步骤流程，首先从各技术主题的专利中提取 SAO 结构，然后利用机器学习对其进行分类，最后汇总以确定主题间的关系。
        * **输入**:
            1.  从 LDA 模型输出的各技术主题下的关键词列表。
            2.  包含这些关键词的原始专利文本。
        * **输出**:
            1.  成对的技术主题之间的主要语义关系类型（构成、改进、替代、应用）。
            2.  可视化的技术路线图（TRM）。
        * **流程**:
            1.  **SAO 提取**: 专家为每个技术主题挑选核心术语，然后使用这些术语从专利中提取相关的 SAO 结构。
            2.  **关系分类**: 专家定义四种语义关系类型（构成、改进、替代、应用），并为一小部分 SAO 样本（10%）进行手动标注。
            3.  **模型训练**: 使用标注好的样本训练一个朴素贝叶斯分类器，使其能够自动识别 SAO 结构对应的关系类型。
            4.  **关系推理**: 将训练好的分类器应用于所有剩余的 SAO 结构，进行自动分类。
            5.  **TRM 构建**: 统计每对主题间不同语义关系的数量，结合专家判断，确定最重要的关系，并在一个分层（材料与结构、组件技术、产品、应用与市场）的图表中绘制出技术路线图。
        * **优势**: 能够揭示技术主题（而非单个词）之间的具体作用关系，使技术演化路径的分析更加深入和直观。

* **重要公式**
    * **困惑度 (Perplexity)**: 用于评估 LDA 模型的拟合优度，值越低越好。
        $$\text{Perplexity}(D) = \exp \left( - \frac{\sum_{d=1}^{M} \log(p(w_d))}{N} \right)$$
        其中，$D$ 是语料库，$M$ 是文档总数，$N$ 是语料库总词数，$p(w_d)$ 是模型生成文档 $d$ 中词语的概率。
    * **总精度 (Total Precision)**: 用于验证主题提取模型的准确性。
        $$\text{Total Precision} = \frac{\text{被正确聚类到其主题的记录数}}{\text{记录总数}}$$
    * **主题权重 (Topic Weight, TW)**: 衡量一个主题在整个语料库中的重要性。
        $$TW_j = \sum_{i=1}^{M} p_{ij}$$
        其中，$p_{ij}$ 是第 $i$ 个文档在第 $j$ 个主题上的概率分布值。
    * **主题趋势 (Topic Trend, TT)**: 衡量一个主题的潜力，通过计算近年主题权重的年均增长率得出。
        $$TT_j = \frac{TW_j^{2018} + TW_j^{2017} + TW_j^{2016}}{TW_j^{2015} + TW_j^{2014} + TW_j^{2013}}$$

### 3. 实验设计与结果（含创新点验证）

* **实验流程**
    1.  **数据收集与预处理**: 从 DII 数据库中检索了 1991 年至 2019 年间与 DSSC 相关的 9,883 项专利。使用 VantagePoint 软件提取标题、摘要、IPC 和 DMC，并通过词语聚类（term clumping）和去除低频项进行数据清洗，最终得到 7,482 个术语、2,554 个 IPC 和 3,088 个 DMC。
    2.  **优化主题提取模型的验证**:
        * **构建训练集**: 手动筛选 2011-2012 年的 DSSC 专利，得到一个包含 821 项记录、涉及 5 个高度耦合的 DSSC 子技术的标注训练集。
        * **确定主题数K**: 对 8 个模型在主题数 K 从 5 到 50 的范围内进行测试，计算平均困惑度。结果显示，当 K=50 时，所有模型的困惑度最低，因此选择 K=50。
        * **模型性能对比**: 将 8 个模型分别应用于训练集，计算总精度（TP）。
    3.  **核心与潜力技术识别**:
        * 将性能最佳的模型（B-#4）应用于全部 9,883 项专利，提取出 50 个主题。
        * 经专家筛选和合并，最终得到 33 个有意义的技术主题。
        * 计算这 33 个主题的 TW（重要性）和 TT（潜力）值。
        * 绘制重要性-潜力二维散点图，将主题分为四个象限。
    4.  **技术路线图构建**:
        * 为这 33 个主题提取了总计 28,815 个 SAO 结构。
        * 随机抽取 10% (2,882 个) 作为训练集，训练朴素贝叶斯分类器来识别四种语义关系。该分类器达到了 82% 的准确率，优于决策树（77%）等其他算法。
        * 用训练好的模型对剩余 SAO 进行分类，并结合专家意见，绘制了 DSSC 的技术路线图。

* **数据集、参数、评价指标**
    * **数据集**: 9,883 项 DII 专利（DSSC），一个 821 项的标注训练集，以及一个用于对比的低耦合度多技术领域数据集。
    * **参数**: LDA 模型中，超参数 $\alpha=0.5, \beta=0.1$；Gibbs 采样迭代 2000 次；主题数 K=50；标题权重为摘要的 2 倍。
    * **评价指标**: 困惑度（Perplexity）、总精度（Total Precision, TP）、主题权重（TW）、主题趋势（TT）。

* **创新点如何得到验证，结果对比与可视化描述**
    * **创新点一：优化的主题提取模型**
        * **验证**: 通过对比 8 个模型的总精度（TP）来验证。
        * **结果对比**:
            1.  **加权标题的有效性**: B 组模型（加权标题）的 TP 全面高于 A 组模型，证实为标题术语赋予更高权重能显著提升主题识别准确率。例如，B-#4 (71.06%) 高于 A-#4 (69.21%)。
            2.  **分类码的有效性**:
                * 对于高度耦合的数据（DSSC 案例），直接加入 IPC/DMC 效果不明显。但去除掉各个子技术共享的通用分类码后，TP 得到提升（例如，B-#4 的 TP 从 71.06% 提升到 74.73%）。这表明分类码在去除共性噪声后能有效提升分类精度。
                * 在低耦合度数据集上，加入 IPC/DMC 的模型（如 B-#4: 83.10%）相比不加的模型（B-#1: 70.01%）TP 提升非常显著。
            3.  **IPC vs. DMC**: 在低耦合数据和处理后的高耦合数据中，包含 DMC 的模型（B-#3）比包含 IPC 的模型（B-#2）表现更好，说明 DII 数据库提供的 DMC 分类更精细，对主题识别更有利。
            4.  **IPC+DMC**: B-#4 模型（结合 IPC 和 DMC）在所有测试中均表现最佳，证明了两者结合使用的优越性。
        * **可视化**: 图 4 展示了不同模型在不同主题数下的困惑度曲线，图表清晰地显示了 K=50 是最佳选择。

    * **创新点二：结合 SAO 和主题模型的语义关系分析**
        * **验证**: 通过成功构建出详细且有解释力的技术路线图（TRM）来验证。
        * **结果与可视化描述**:
            * 图 5 的散点图直观展示了 33 个技术主题在重要性和潜力两个维度上的分布，清晰地划分出核心潜力区（I象限）、高潜力区（II象限）等。例如，“建筑”、“LED”、“光阳极”等主题位于 I 象限，是核心和潜力兼备的技术。
            * 图 6 的技术路线图（TRM）是最终成果，它不仅展示了不同层级的技术（材料->组件->产品->应用），还用箭头清晰地标示了技术间的演化路径和语义关系（如`Compose`, `Improve`, `Replace`, `Apply`）。例如，图中清晰地展示了电解质从`液体电解质`被`固体电解质`所`替代 (Replace)`，以及`碳基电极`替代`金属基电极`的演化路径。

* **主要实验结论与作者解释**
    * 作者得出结论，B-#4 模型（加权标题 + 摘要 + IPC + DMC）是用于 DII 专利主题识别的最佳模型。
    * 对于主题高度耦合的数据，需要先剔除各主题共有的分类码，才能发挥分类码在提升识别精度方面的作用。
    * 通过构建的 TRM，作者识别出 DSSC 技术的 5 条核心演进路径（电解质、基底、对电极、光阳极、敏化染料）和多个技术机会，例如，固态电解质、TiO2 基光阳极和有机染料是组件技术中的主导方向，而建筑和 LED 是最具潜力的应用市场。

### 4. 研究结论

* **重要发现（定量 / 定性）**
    * **定量发现**:
        1.  通过优化特征，主题识别的总精度（TP）最高可达 74.73%（对于高耦合数据）和 83.10%（对于低耦合数据）。
        2.  在 DSSC 技术中，“建筑”应用的重要性（TW=247）和潜力（TT=124.52%）均非常高。
    * **定性发现**:
        1.  专利标题比摘要包含更多核心信息，给予更高权重是有效的。
        2.  DII 专利的 DMC 分类码比传统 IPC 在技术主题识别上更具优势。
        3.  DSSC 技术的演化趋势呈现出向固态化（固态电解质）、柔性化（柔性基底）、低成本化（碳基电极替代贵金属电极）和有机化（有机染料）发展的明确路径。
        4.  TiO2 是制造多孔金属氧化物半导体的最有前途的材料，其发展方向包括改进纳米结构和碳材料掺杂。

* **对学术或应用的意义**
    * **学术意义**: 提出了一种新颖的、系统性的技术机会分析框架，该框架通过优化主题提取的准确性并深入挖掘主题间的语义关系，补充了现有文献的不足。研究为专利计量学和文本挖掘领域提供了关于如何更有效地利用专利数据（特别是 DII 数据）的新见解。
    * **应用意义**: 为企业、政府和投资机构提供了一个强大的、数据驱动的决策支持工具。决策者可以利用该方法识别新兴技术领域中的核心技术、潜力技术及其演化路径，从而制定更精准的研发战略、投资规划和技术预见。

### 5. 创新点列表

* **提出优化的专利主题提取模型**: 首次系统性地研究并证实，通过结合“位置加权”（赋予专利标题更高权重）和“分类码融合”（同时使用 IPC 和 DMC），可以显著提高技术主题提取的准确性。
* **开发主题层面的语义关系分析方法**: 独创性地将主题模型、SAO 技术和机器学习相结合，实现了从“点对点”的技术关系分析到“主题对主题”的技术簇关系分析的跨越，能够揭示更宏观、更具战略意义的技术演化逻辑。
* **构建了系统的混合分析框架**: 整合了从数据处理、模型优化、技术识别到路线图构建的完整四阶段流程，为技术机会分析提供了一套可复现、高精度的系统化方法论。
* **提供了关于专利分类码使用的新洞见**: 通过对比实验，深入分析了数据耦合度对 IPC 和 DMC 效用的影响，并得出在处理高度相关的技术专利时，需对共享分类码进行预处理以提升模型性能的实用结论。

=============================《文章分隔符》=============================

# Combining deep neural network and bibliometric indicator for emerging research topic prediction - April 2021

### 1. 研究目标 · 内容 · 问题 · 出发点

* **研究领域与背景、具体对象 / 数据集**
    * **研究领域与背景**：本研究属于文献计量学和信息科学领域，专注于新兴研究主题的预测。预测新兴主题对于科研人员、政策制定者和资助机构至关重要，能为科研方向和资金分配提供洞见。然而，以往多数研究是回顾性的，即识别已经涌现的主题，而非前瞻性地预测未来可能的热点。
    * **具体对象 / 数据集**：研究使用了两个从 Web of Science 收集的文献数据集：
        1.  **基因编辑 (Gene editing)**：包含 2003 年至 2018 年间的 19,164 篇出版物。
        2.  **移植 (Transplant)**：包含 2003 年至 2018 年间 25 种移植领域期刊的 206,573 篇出版物。

* **论文想解决的核心问题**
    * 如何从现有的海量科学文献中，有效地、前瞻性地预测未来将要兴起的研究主题？论文旨在解决传统方法依赖回顾性分析和预设规则、缺乏预测能力的局限性。

* **研究动机 / 假设**
    * 作者假设，一个结合了机器学习和文献计量学指标的混合方法，能够更有效地预测新兴研究主题。其核心思想是：
        1.  新兴主题兼具“新颖性 (novelty)”、“成长性 (growth)”和“影响力 (impact)”三大特征。
        2.  深度神经网络（如 LSTM）能够捕捉研究主题发展过程中的非线性动态，适合用于预测其未来的“成长性”与“影响力”。
        3.  文献计量学指标可以作为一种有效的规则，从被预测为高热度的候选项中筛选出具备“新颖性”的主题。

* **工作内容概览**
    * 论文提出并验证了一个用于预测新兴研究主题的两步式解决方案。
    * **第一步：预测热度**。将主题的“成长性”和“影响力”合并为一个名为“热度分 (Popularity Score)”的指标。然后将该问题形式化为一个多元多步时间序列预测任务，使用深度学习模型（LSTM 和 NNAR）基于历史数据预测候选主题未来的热度分。
    * **第二步：筛选新颖性**。在第一步预测出的高热度主题中，应用一组基于文献计量学的规则（新颖性指标）来过滤出真正具备“新颖性”特征的主题，最终提名其为新兴主题。
    * **实验与验证**：在“基因编辑”和“移植”两个数据集上进行了全面的实验，将所提方法与多种基线模型（如 LightGBM、线性/多项式回归）和训练策略（全局 vs. 局部）进行对比，并通过定量指标（MAE, RMSE, NDCG@20）和定性分析（文献综述验证）证明了方法的有效性。

### 2. 研究方法（含模型 / 技术详解）

* **理论框架与算法**
    * 研究的整体框架是一个两步走的混合预测流程：
        1.  **候选主题生成**：使用 `Termolator` 工具从特定领域（前景数据集）和通用领域（背景数据集）的文献中自动抽取出具有领域特征的名词短语，作为候选研究主题。
        2.  **第一步：热度预测（机器学习）**：将问题建模为时间序列预测。利用深度学习模型，基于候选主题过去多年的多维度特征，预测其未来多年的热度分。此步旨在识别未来可能“高成长、高影响”的主题。
        3.  **第二步：新颖性过滤（文献计量学）**：对第一步的预测结果，应用一个包含三条规则的“新颖性指标”进行筛选，确保最终被提名的主题不仅未来热度高，而且在其发展初期表现出典型的“新颖”模式。

* **关键模型/技术逐一说明**
    * **Termolator (候选主题生成)**
        * **架构**：一个高性能的术语提取程序。
        * **流程**：输入一个领域特定的前景语料库和一个通用的背景语料库，通过一系列统计指标来衡量名词短语的领域特异性，并为其打分。得分高的短语被视为候选研究主题。
        * **优势**：相比需要人工标注的聚类或主题模型，该方法自动化程度高，干预少。

    * **深度学习预测模型 (LSTM & NNAR)**
        * **输入**：每个主题的多元时间序列数据。序列长度为 11 年，每个时间点（年）包含 9 个特征：**热度分 (Popularity Score)**、**文档频率 (DF)**、**词频 (TF)**、**学术关注度 (Academic focus)**、**政府关注度 (Governmental focus)**、**知识基础 (Intellectual Base)**、**关键词 (Keywords)**、**KeywordPlus** 和 **知识范围 (Knowledge scope)**。
        * **输出**：预测该主题未来 5 年的热度分序列。
        * **LSTM (长短期记忆网络)**：
            * **架构**：一种循环神经网络（RNN），通过其独特的门控单元（输入门、遗忘门、输出门）有效处理和记忆时间序列中的长期依赖关系。本研究中使用“序列到序列 (sequence-to-sequence)”结构。
            * **优势**：能够显式地处理数据的时间依赖性，保留序列顺序信息，非常适合捕捉主题发展的复杂非线性模式。
        * **NNAR (神经网络自回归)**：
            * **架构**：一种前馈神经网络，将历史时间点上的目标值和特征值作为输入，来预测当前时间点的目标值。
            * **优势**：能够捕捉非线性关系，且不要求时间序列是平稳的。
            * **局限**：将所有历史输入“扁平化”处理，丢失了 LSTM 所能保留的原始时序顺序和依赖关系。

* **重要公式（如有）**
    * **热度分 (Popularity Score)**：作为预测目标的核心指标，结合了影响力和增长率。
        $$\text{Popularity Score} = \ln(\text{ADF} + \delta) \times \frac{\text{DF}_i + \delta}{\text{DF}_{i-1} + \delta}$$
        其中，$\text{DF}_i$ 是主题在第 $i$ 年的文档频率。$\text{ADF}$ (Adjusted Document Frequency, 调整后文档频率) 用于衡量累积影响力，计算方式为：
        $$\text{ADF}_i = \text{DF}_i + \alpha \times \text{ADF}_{i-1}$$
        这里的 $\alpha$ 是一个衰减因子（本研究设为 0.9），控制历史影响力的衰减速度；$\delta$ 是一个平滑因子（设为 1.0），以避免分母为零。

    * **新颖性指标 (Novelty Indicator)**：用于第二步筛选的三个规则。
        1.  **早期频率**：主题在头 3 年的文档频率低于所有主题的第 15 百分位数。
        2.  **后期频率**：在随后的 8 年里，其文档频率至少是头 3 年的两倍。
        3.  **持续性**：在随后的 8 年里，至少有 4 年出现过。

### 3. 实验设计与结果（含创新点验证）

* **实验流程**
    1.  **数据准备**：从 Web of Science 收集“基因编辑”和“移植”两个领域 2003-2018 年的文献数据。
    2.  **主题与特征提取**：使用 Termolator 提取候选主题，并为每个主题计算 2003-2018 年间每年的 9 个特征值。
    3.  **模型训练与预测**：使用 2003-2013 年的数据作为训练集，来预测 2014-2018 年的热度分。
    4.  **策略对比**：
        * **全局策略**：用一个数据集中的所有主题训练一个模型。
        * **局部策略**：将主题分组（按热度高低或趋势升降），为每组单独训练模型。
    5.  **模型评估**：将预测结果与 2014-2018 年的真实值进行比较，计算评价指标。
    6.  **新兴主题提名与定性验证**：使用表现最佳的模型在 2008-2018 年的完整数据上训练，预测 2019-2023 年的热点。对预测结果应用新颖性过滤器，得到最终提名列表，并通过文献综述对排名前 20 的主题进行定性分析。

* **数据集、参数、评价指标**
    * **数据集**：“基因编辑”（2,643 个候选主题）和“移植”（5,141 个候选主题）。
    * **基线模型**：LightGBM、线性回归 (LR)、多项式回归 (PR)、EScore (一种基于规则的评分)、朴素方法 (Naïve, 直接复制前一年的值)。
    * **参数**：热度分公式中，衰减因子 $\alpha = 0.9$，平滑因子 $\delta = 1.0$。神经网络的层数、神经元数等超参数通过 5 折交叉验证进行寻优。
    * **评价指标**：
        * **MAE (平均绝对误差)** 和 **RMSE (均方根误差)**：衡量预测值与真实值的差距，误差越小越好。
        * **NDCG@20 (归一化折损累计增益@20)**：衡量模型对前 20 个热门主题的排序质量，值越接近 1 越好。

* **创新点如何得到验证，结果对比与可视化描述**
    * **混合方法的有效性验证**：最终被提名的主题列表（表6和表7）是对该方法最直接的验证。定性分析显示，列表包含了领域内公认的重要主题，如“CRISPR”（后续获诺贝尔奖）、“ECMO”和“RT-PCR”（在新冠疫情中发挥关键作用），证明了该方法能够发现具有真实世界意义的新兴主题。
    * **模型性能对比 (全局策略)**：
        * **定量结果**：在 MAE 和 RMSE 指标上，LSTM 和 NNAR 显著优于所有基线模型，表明深度学习模型在精确预测热度分数值方面能力更强。然而，在 NDCG@20 指标上，LightGBM 表现最佳，说明它更擅长准确地为顶尖热门主题排序。
        * **可视化描述 (图4)**：展示了各模型在预测期内每年的误差变化。可以看出，对于动态性更强的“基因编辑”数据集，预测时间越远，误差越大。
        * **可视化描述 (图5)**：对比了 LSTM 和 LightGBM 对具体主题的预测曲线。LSTM 的预测更平滑且贴近真实趋势，尤其在处理低分主题时；而 LightGBM 的预测则波动较大。
    * **训练策略对比**：实验结果表明，局部策略并未比全局策略带来显著性能提升。作者解释这可能是因为深度学习模型需要大量样本，而全局策略能利用更多数据进行训练，从而获得更好的泛化能力。

* **主要实验结论与作者解释**
    * 深度神经网络（特别是 LSTM）最适合全面、准确地预测所有研究主题的热度发展轨迹。
    * 基于决策树的 LightGBM 模型虽然整体预测误差较大，但在识别和排序顶尖热门主题方面具有独特优势。
    * 对于此类预测任务，采用全局训练策略（用所有数据训练一个模型）通常比将数据分割后训练多个局部模型的策略更优。
    * 定性分析证实，本研究提出的两步法能够成功提名真实世界中重要的新兴研究方向。

### 4. 研究结论

* **重要发现（定量 / 定性）**
    * **定量**：在预测新兴主题热度方面，深度学习模型（LSTM, NNAR）在预测精度（MAE, RMSE）上全面领先于传统回归、规则方法和梯度提升树模型（LightGBM）。然而，LightGBM 在对热门主题的排序能力（NDCG@20）上表现最佳。
    * **定性**：通过将“热度预测”与“新颖性过滤”相结合，该方法成功地在“基因编辑”和“移植”两个领域中提名了多个关键新兴技术和主题，如 CRISPR、TALENs、ECMO 等。这些主题的重要性得到了后续科学发展和重大奖项（如诺贝尔奖）的印证。

* **对学术或应用的意义**
    * **学术意义**：为新兴主题预测这一复杂问题提供了一个清晰、可行的计算框架。它将问题分解为可被机器学习解决的“热度预测”子任务和可被文献计量学规则处理的“新颖性过滤”子任务，为该领域的研究提供了新的范式。
    * **实践意义**：提供了一套可操作的方法，可以被集成到系统中，供科研人员、资助机构和政策制定者使用。该系统能够实时监测和预测特定领域的研究趋势，为科研布局、资金分配和战略规划提供数据驱动的决策支持。

### 5. 创新点列表

* **提出混合预测框架**：创新性地提出了一个两步走的混合框架，有机地结合了机器学习方法（用于预测“成长性”与“影响力”）和文献计量学指标（用于保证“新颖性”），全面地解决了新兴主题多维度的特性。
* **构建“热度分”指标**：设计并提出了一个全新的预测目标指标——“热度分 (Popularity Score)”，该指标巧妙地融合了主题的累积衰减影响（通过调整后文档频率 ADF 实现）和即时增长率。
* **问题形式化定义**：首次将新兴主题的热度预测问题，在数学上严谨地形式化为一个“多元多步时间序列预测问题”，为使用复杂的机器学习模型解决该问题奠定了理论基础。
* **深度学习的应用与评估**：系统性地将 LSTM 和 NNAR 等现代深度学习技术应用于该预测任务，并通过与多种基线模型的全面对比，验证了它们在捕捉主题演化非线性动态方面的优越性。

=============================《文章分隔符》=============================

# Topics emerged in the biomedical field and their characteristics (2022)

### 1. 研究目标 · 内容 · 问题 · 出发点

- **研究领域与背景、具体对象 / 数据集**
    - **研究领域与背景**：本研究属于科学计量学和信息科学领域，专注于生物医学领域内新兴研究主题的识别与预测。传统研究多采用“出版物视角”（publication perspective），即通过分析文献集合的增长、引用等外部指标来识别新兴主题。本文提出并采用一种“主题视角”（topic perspective）作为补充。
    - **具体对象 / 数据集**：研究对象为2001年至2010年间新加入到生物医学主题词表（Medical Subject Headings, MeSH）中的主题词。数据源于美国国家医学图书馆（NLM）的MeSH官方发布列表和PubMed数据库。经过筛选，最终选取了1279个新的MeSH词条作为分析样本。

- **论文想解决的核心问题**
    - 为什么在生物医学领域，有些新主题能够兴起并保持热度，而另一些则未能兴起？
    - 是否可以仅根据一个新主题的内在特征（inherent characteristics），而非其发表后的外部指标（如增长率、引用模式），来预测其未来的发展趋势？

- **研究动机 / 假设**
    - **研究动机**：现有新兴主题预测研究侧重于“如何识别”，较少关注“为何兴起”，且往往忽略了那些有潜力但最终未能兴起的主题（反事实样本）。本研究旨在通过分析主题自身的属性，来解释其兴起的驱动因素，填补这一空白。
    - **核心假设**：一个主题的未来流行度，取决于它自身的内在特征。具体假设：主题的类别、临床重要性、及其在MeSH层级结构中的位置（即引入时是否自带下位词）会显著影响其未来的流行程度。

- **工作内容概览**
    - **引言**：提出研究问题，并阐明本研究采用的“主题视角”与传统“出版物视角”的区别和互补性。
    - **文献综述**：回顾了基于关键词频率、文献聚类和机器学习等新兴主题识别方法，指出其局限性，并强调了本研究在视角和方法上的独特性。
    - **方法**：详细说明了数据收集和筛选流程。定义了三个核心的“主题特征”（主题类别、临床显著性、有无下位词）。定义了四种新兴趋势模式，并构建了基于逻辑回归的预测模型。
    - **结果**：对数据进行描述性统计分析。通过统计检验，验证了三个主题特征对流行度的影响。分析了四种趋势模式的分布，并评估了预测模型在不同时间点的表现。
    - **讨论与结论**：总结了主要发现，探讨了研究的贡献、局限性，并强调了在未来研究中考虑主题和领域特征的重要性。

### 2. 研究方法（含模型 / 技术详解）

- **理论框架与算法**
    本研究的理论框架是“主题视角”，即认为一个主题的内在属性（根植于领域知识）是其能否兴起的根本驱动力。研究方法主要结合了**描述性统计分析**、**非参数统计检验**和**监督式机器学习（逻辑回归）**。

- **关键模型/技术逐一说明**
    1.  **主题特征量化**：
        - **主题类别 (Topic Category)**：根据MeSH的16个顶级层级（A-解剖学, B-生物, C-疾病, D-化学品和药物等）对主题进行分类。这是一个分类变量。
        - **临床显著性 (Clinical Significance)**：一个二元变量。如果在PubMed中，某个MeSH词条被用于索引至少一篇“临床试验 (Clinical Trial)”类型的文章，则认为该主题具有临床显著性。
        - **有无下位词 (Narrower Terms)**：一个二元变量。考察一个MeSH词条在被收录的当时，其层级结构下是否存在更具体的下位词。这用于区分真正的新概念和为完善层级结构而增设的上位概念。

    2.  **新兴趋势模式识别**：
        - 基于规则的分类方法，用于刻画主题随时间变化的流行度模式。
        - **“兴起”的定义**：在任何一年，被某个MeSH词条索引的文章数量达到或超过25篇（此阈值为数据集中“年均索引文章数”的第三四分位数）。
        - **四种模式**：
            - **兴起并持续 (Emerged and Sustained)**：达到兴起阈值后，其热度基本维持在该水平之上（允许连续少于两年的短暂下降）。
            - **兴起但未持续 (Emerged not Sustained)**：达到阈值后，连续两年或以上降至阈值以下，且未再兴起。
            - **兴起并波动 (Emerged and Fluctuated)**：达到阈值后，热度下降，之后再次达到阈值。
            - **尚未兴起 (Not yet Emerged)**：从未达到兴起阈值。

    3.  **预测模型 (Logistic Regression)**：
        - **架构**：使用逻辑回归模型来预测一个新MeSH词条未来是否会成为“新兴主题”。
        - **输入 (Predictors)**：三个主题特征变量（主题类别、临床显著性、有无下位词）。主题类别使用虚拟编码（Dummy Coding）处理。
        - **输出 (Target)**：一个二元变量，表示“是否为新兴主题”。“新兴主题”被定义为在其同年加入的MeSH词条中，总索引文章数排名前25%（Q4）的主题。
        - **训练流程**：为解决数据不平衡问题（新兴主题占25%，非新兴占75%），在训练集中对多数类（非新兴主题）进行了下采样（Down Sampling）。模型性能评估采用五折交叉验证。
        - **重要公式**：
            - 逻辑回归模型估计第 $i$ 个MeSH词条成为新兴主题的概率 $\pi_{i}$：
              $$\pi_{i}=\frac{e^{Z_{i}}}{1+e^{Z_{i}}}$$
            - 其中 $Z_{i}$ 是预测变量的线性组合：
              $$Z_{i}=\beta_{0}+\beta_{1}X_{i1}+\beta_{2}X_{i2}+\beta_{3}X_{i3}+\epsilon_{i}$$
            - 在这个模型中，$X_{i1}$ 对应“临床显著性”，$X_{i2}$ 对应“有无下位词”，$X_{i3}$ 对应“主题类别”。

### 3. 实验设计与结果（含创新点验证）

- **实验流程**
    1.  **数据收集与清洗**：从NLM官方FTP获取2001-2010年新增MeSH列表。通过MeSH RDF数据接口和NCBI E-utilities查询PubMed数据库，对词条进行筛选：
        - 排除已被删除的词条。
        - 排除带有“Previously Indexing”标签的词条（表明是旧概念换名）。
        - 排除其首次索引的文章发表日期早于其被收录日期五年以上的词条（表明是已存在的概念）。
        - 排除不用于表征主题内容的V（出版特征）和Z（地理）类别。
        - 最终得到1279个有效的新MeSH词条。
    2.  **流行度与特征测量**：对每个词条，统计截至2019年底在PubMed中索引的文章总数（不包括其下位词索引的文章）作为其流行度。同时，确定其主题类别、临床显著性、以及收录时有无下位词这三个特征。
    3.  **统计分析**：
        - 使用卡方检验（Chi-square test）分析“主题类别”与“流行度四分位”（Q1-Q4）之间的独立性。
        - 由于流行度数据呈严重偏态分布，使用非参数的克鲁斯卡尔-沃利斯检验（Kruskal-Wallis test）比较不同二元特征分组（如“有/无临床显著性”）的流行度中位数是否存在显著差异。
    4.  **趋势模式分类**：将1279个词条按前述规则划分为四种趋势模式，并分析不同主题类别在这些模式中的分布。
    5.  **预测实验**：
        - **目标定义**：将每个年份队列中流行度排名前25%（Q4）的词条标记为“新兴”，其余为“非新兴”。
        - **模型训练与评估**：为模拟预测的及时性，实验分别在词条加入后的第1年到第10年进行。在第M年的预测中，只使用截至该年可获得的信息（这主要影响“临床显著性”指标的获取）。采用五折交叉验证训练逻辑回归模型，并使用准确率、精确率、召回率、F1值和关键成功指数（Critical Success Index, CSI）作为评价指标。

- **数据集、参数、评价指标**
    - **数据集**：1279个于2001-2010年新增的MeSH词条。
    - **关键参数**：
        - 趋势模式的“兴起”阈值：年索引文章数 ≥ 25篇。
        - 预测模型的“新兴”定义：同批次词条中总流行度排名前25% (Q4)。
    - **评价指标**：准确率 (Accuracy), 精确率 (Precision), 召回率 (Recall), F-measure, 关键成功指数 (CSI = TP / (TP+FP+FN))。CSI对假阳性和假阴性都进行惩罚，更适合不平衡预测任务的评估。

- **创新点如何得到验证，结果对比与可视化描述**
    - **验证“主题特征影响流行度”的假设**：
        - **主题类别**：卡方检验结果显著（p < 0.01）。关联图（Mosaic Plot）直观显示，B类（生物）更倾向于出现在最低流行度分区（Q1），而E类（技术与设备）和G类（现象与过程）更倾向于出现在最高流行度分区（Q4）。
        - **临床显著性**：Kruskal-Wallis检验结果显著（p < 0.01）。箱形图（Boxplot）显示，有临床显著性词条的流行度中位数（272）远高于无临床显著性的词条（45）。
        - **有无下位词**：Kruskal-Wallis检验结果显著（p < 0.01）。箱形图显示，无下位词的词条流行度中位数（122）显著高于有下位词的词条（77）。
    - **验证“基于主题特征的可预测性”的假设**：
        - **预测性能**：预测模型的性能优于随机猜测。CSI得分随预测时间推移而提高，从第1年的27.47%上升到第6年达到峰值40.34%，之后趋于平稳。这表明随着时间推移（特别是临床试验信息的出现），预测能力增强。
        - **可视化**：通过折线图展示了CSI得分随预测年份（1-10年）的变化趋势，清晰地标示出第3年和第6年的关键性能水平。
    - **趋势模式分析**：
        - **分布**：发现大部分（62.15%）新主题“尚未兴起”。在兴起的主题中，约65%能够“兴起并持续”。
        - **与主题类别的关联**：卡方检验表明趋势模式与主题类别显著相关。例如，J类（技术、工业、农业）的新词条绝大多数（11/13）都属于“兴起并持续”模式。

- **主要实验结论与作者解释**
    - 主题的内在特征确实是其未来流行度的重要驱动因素。
    - 与生物医学研究核心使命（改善人类健康）直接相关的特征，如具有临床应用前景（临床显著性）、代表新颖的技术或过程，更容易获得关注并成为热门主题。
    - 结构性添加的词条（即带有下位词的上位词）由于索引规则要求使用最具体的词，其自身流行度会受限。
    - 预测模型证明，仅利用这些早期可知的内在特征，就能在一定程度上预测一个主题的潜力。性能随时间提升，反映了信息（如临床试验结果）逐步明确的过程。

### 4. 研究结论

- **重要发现（定量 / 定性）**
    - **定性发现**：
        1.  **视角验证**：“主题视角”是有效且必要的，它揭示了新兴主题兴起的内在驱动力。
        2.  **特征影响**：主题的类别、临床显著性、以及在知识结构中的位置（有无下位词）是影响其未来发展的关键因素。
        3.  **领域特性**：生物医学领域的新兴主题趋势与其领域使命高度相关，即与治疗疾病、改善健康直接相关的主题更易成功。
        4.  **趋势多样性**：主题兴起并非简单的二元过程，而是呈现出“兴起并持续”、“兴起但未持续”、“兴起并波动”和“尚未兴起”四种不同模式。
    - **定量发现**：
        1.  具有临床显著性的主题比没有的流行得多（中位数272 vs. 45）。
        2.  在加入时没有下位词的主题比有的更流行（中位数122 vs. 77）。
        3.  在所有兴起的主题中，约65%能够维持其热度。
        4.  基于主题特征的预测模型，其CSI（关键成功指数）在第6年达到峰值（40.34%），表明了在主题出现后的数年内可实现有效预测。

- **对学术或应用的意义**
    - **学术意义**：
        - 提出并验证了一种研究新兴主题的新视角（主题视角），补充了现有文献。
        - 通过纳入失败样本（未兴起的主题）进行对比分析，克服了以往研究的局限性。
        - 强调了领域分析（domain analysis）的重要性，证明了普适性模型之外，考虑特定领域特征的价值。
    - **应用意义**：
        - 为科研资助机构、科技政策制定者和信息专业人员提供了一种新的评估工具，帮助他们在新主题出现的早期，基于其内在属性，更准确地判断其发展潜力，从而做出更优的资源分配决策。

### 5. 创新点列表

1.  **提出并验证“主题视角”**：首次系统地从主题自身的内在特征（而非外部文献计量指标）出发，来研究和预测新兴主题的演化，为该领域提供了新的研究范式。
2.  **纳入反事实样本进行分析**：通过追踪所有新加入的MeSH词条，本研究能够直接比较成功兴起的主题与那些有潜力但最终失败的主题，使得对“兴起”驱动因素的分析更为全面和可靠。
3.  **识别并量化了新的预测指标**：明确提出“主题类别”、“临床显著性”和“有无下位词”作为预测新兴主题的有效指标，这些指标根植于领域知识，具有很强的解释力。
4.  **揭示了多样的兴起趋势模式**：超越了传统的“新兴/非新兴”二元划分，识别并分析了四种具体的动态演化路径，深化了对主题生命周期的理解。
5.  **强调并实践了领域特定分析**：通过深入分析生物医学领域的案例，证明了新兴主题的驱动因素与特定领域的使命和知识结构紧密相连，倡导在科学计量学研究中进行更具针对性的领域分析。
